# 设计文档

下面是一份**可直接用于开工**的《刷牙游戏（Web MVP）设计文档》，目标是：**先用 Web 最快验证**（MediaPipe Face Landmarker / Hand Landmarker，类似 Pokemon Smile 的交互节奏），后续**同一套核心逻辑平滑迁移到 iOS/Android/桌面端**（MediaPipe Tasks 跨平台）。([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))

---

## 1. 项目目标与范围

### 1.1 目标（MVP）

实现如下闭环：

1. 进入游戏 → 选择卡通头套
2. 点击开始 → 打开摄像头 → 头套实时跟随人脸
3. 提示张嘴露牙 → **识别“露出牙齿/张嘴到位”才可进入下一步**
4. 提示“手部刷牙动作” → **单手握拳快速晃动**识别通过
5. 通过后判定“完成刷牙”并加积分

### 1.2 非目标（MVP 先不做）

- 刷牙时长分区/上下左右分区刷牙质量评分（后续迭代）
- 精准“牙齿像素级可见”识别（MVP 用“张嘴到位/口部开合”代理；可扩展为轻量分类器）

---

## 2. 技术选型与依据

### 2.1 核心 SDK

- **MediaPipe Tasks Vision**
    - **Face Landmarker（Web JS）**：人脸关键点 + 表情/Blendshapes（用于嘴部开合等信号）+ 支持做滤镜/虚拟形象的能力([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
    - **Hand Landmarker（Web JS）**：手部 21 关键点 + handedness + world 坐标，有利于做“握拳 + 晃动速度”判断([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker?utm_source=chatgpt.com))

### 2.2 Web 端推理形态

- MediaPipe 可在浏览器以 **WebAssembly** 方式跑实时管线（适合隐私、低延迟、免后端）([谷歌开发者博客](https://developers.googleblog.com/en/mediapipe-on-the-web/?utm_source=chatgpt.com))

### 2.3 商用与许可

- MediaPipe 仓库许可为 **Apache 2.0**（允许商用、修改与分发，按许可条款保留声明即可）([GitHub](https://github.com/google-ai-edge/mediapipe/blob/master/LICENSE?utm_source=chatgpt.com))

---

## 3. 体验与交互流程（状态机）

### 3.1 状态机定义

- `S0_SelectAvatar`：选择头套（本地资源列表）
- `S1_CameraInit`：请求摄像头权限、加载模型
- `S2_FaceTracking`：头套实时跟随（持续）
- `S3_PromptTeeth`：提示“张嘴露牙”
- `S4_TeethConfirmed`：露牙/张嘴到位判定通过（进入下一步）
- `S5_PromptBrushGesture`：提示“单手握拳快速晃动”
- `S6_BrushGestureConfirmed`：动作通过 → 完成一次刷牙
- `S7_Completed`：展示积分、动效、下一局/退出

> 关键点：人脸跟随（S2）是常驻子系统，不要和露牙/手势判定耦合在一起，避免状态切换导致滤镜抖动。
> 

---

## 4. 系统架构（Web MVP）

### 4.1 模块划分

**(A) UI 层**

- Avatar 选择页（头套列表、预览）
- 游戏主界面（摄像头画面、提示文案、进度/积分、调试开关）

**(B) 视觉推理层（MediaPipe）**

- `FaceTracker`
    - 输入：视频帧
    - 输出：face landmarks、blendshapes、（可选）face transform matrix（用于更稳的头套贴合）([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
- `HandTracker`
    - 输入：视频帧
    - 输出：hand landmarks（image/world）、handedness([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker?utm_source=chatgpt.com))

**(C) 判定与游戏逻辑层**

- `TeethGate`（露牙/张嘴到位门槛）
- `BrushGestureDetector`（握拳 + 快速晃动）
- `GameStateMachine`（推进状态、积分、节奏）

**(D) 渲染层**

- 视频渲染 `<video>`
- 覆盖渲染 `<canvas>`：头套、关键点调试、提示圈等

### 4.2 数据流

`getUserMedia()` → VideoFrame → (FaceLandmarker + HandLandmarker) → 结果 → (TeethGate / BrushGestureDetector) → StateMachine → UI & Canvas Render

---

## 5. 核心算法设计

## 5.1 头套实时跟随（人脸滤镜）

**目标**：头套在游戏全过程跟随人脸位置、尺度、旋转。

**推荐实现（MVP 可选难度）：**

- 简版（最快）：
    - 取脸部关键点（例如眼睛/鼻尖/额头附近）计算：
        - 位置：鼻尖或两眼中心
        - 尺度：两眼距
        - 旋转：两眼连线角度（roll）
    - 在 Canvas 上绘制头套 PNG（2D 仿射变换）
- 稳定版（更 AR）：
    - 使用 Face Mesh/Face Landmarker 体系的**face transform / facial transformation matrix**做更稳的贴合（更抗抖、抗角度变化）([GitHub](https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md?utm_source=chatgpt.com))

**抗抖处理**

- 对位置、尺度、角度做一阶低通滤波（EMA）：
    - `smooth = lerp(prev, current, alpha)`，alpha 建议 0.2~0.35
- 当检测丢失 face（N 帧）→ 头套渐隐或锁定上一帧并提示“请对准镜头”

---

## 5.2 “露出牙齿”判定（MVP：张嘴到位 Gate）

> 现实里“牙齿可见”会受光照、肤色、嘴唇遮挡影响。MVP 用“口部开合到位”作为准入，后续再升级为“牙齿像素级可见分类”。
> 

### 方案 A（推荐 MVP）：Blendshapes / Mouth Open 信号

Face Landmarker 支持输出**表情/Blendshapes**([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))

做法：

- 选择能代表张嘴的 blendshape（例如 jawOpen / mouthOpen 类）
- 判定：`score > T_open` 连续稳定 `>= X ms` 才算通过
    - 建议：`T_open = 0.45~0.6`（需机型实测）
    - 稳定时长：300~600ms（防止瞬间误触）

### 方案 B（备选/补充）：关键点 Mouth Aspect Ratio（MAR）

基于唇部关键点计算上下唇距离 / 嘴宽，得到开口比值 MAR：

- `MAR = (dist(upperLip, lowerLip) / dist(mouthLeft, mouthRight))`
- 判定：`MAR > T_mar` 且稳定 X ms

### 失败与引导

- 未通过：提示“张大嘴巴，露出牙齿”
- 通过后：锁定进入下一状态（避免一会儿闭嘴又退回造成体验差）
    - 但如果后续你希望“刷牙过程中必须持续张嘴”，可以加“软约束”：低于阈值只提醒，不打断。

### 后续升级（更像“露牙”而非“张嘴”）

- 取 mouth ROI（基于关键点裁剪），跑一个超轻量二分类模型（TeethVisible/Not）：
    - TFJS/ONNX Runtime Web 都可
    - 训练数据可先用你自己采集 + 少量增强（亮度/对比度/模糊）

---

## 5.3 “刷牙动作”判定：单手握拳快速晃动

### Step 1：单手检测与锁定

- Hand Landmarker 支持多手输出和 handedness([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker?utm_source=chatgpt.com))
    
    规则：
    
- 优先取画面中置信度最高的那只手
- 要求连续检测到手 `>= 300ms` 才进入“手势判定”

### Step 2：握拳（Fist）判定

用 21 点手关键点做规则判定（无需模型）：

- 对每根手指，判断指尖是否“靠近掌心/指根”：
    - `dist(tip, mcp)` 或 `dist(tip, wrist)` 小于阈值（阈值随手大小归一化）
- 例如：至少 4 根手指满足“卷曲”即可认为握拳
- 也可加“拇指卷曲”作为更严格条件（减少张开手误触）

### Step 3：快速晃动（Shake）判定

用 `wrist` 或 `palm center` 的轨迹速度：

- 每帧记录 `pos(t)`（推荐用 world 坐标更稳定）([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker?utm_source=chatgpt.com))
- 计算速度：`v = |pos(t)-pos(t-1)| / dt`
- 在一个滑窗内（例如 800ms）统计：
    - `highSpeedFrames`（v > T_speed 的帧数）
    - 或计算能量：`sum(v^2)`
- 判定通过：
    - “握拳成立” AND “800ms 内 highSpeedFrames 占比 > 35%”
    - 同时可加“方向来回变化次数（过零次数）”以更像“晃动”而不是匀速移动

### 防作弊/鲁棒性

- 手移出画面：重置滑窗
- 单纯把拳头靠近镜头猛移动：可加入“位移幅度上限 + 高频变化”组合约束
- 允许左右手：handedness 不作限制

---

## 6. 关键工程实现（Web）

### 6.1 技术栈建议

- 前端：Vite + React + TypeScript（或纯 TS）
- 渲染：Canvas 2D（MVP 足够）；后续可上 WebGL/Three.js
- MediaPipe：`@mediapipe/tasks-vision`（FaceLandmarker / HandLandmarker）

### 6.2 性能目标（MVP）

- 30 FPS 体验为佳；最低可接受 15 FPS
- 推理频率可“降采样”：
    - UI 渲染每帧
    - Landmarker 每 2 帧/3 帧跑一次（根据设备自适应）
- 降低输入分辨率（例如 640x480）提升速度

### 6.3 相机与隐私

- 仅本地处理（浏览器 wasm 推理，默认不上传视频流）([谷歌开发者博客](https://developers.googleblog.com/en/mediapipe-on-the-web/?utm_source=chatgpt.com))
- UI 明示：需要摄像头权限、用途、是否录制（默认不录制）
- 若要埋点：只上传“事件数据”（通过/失败/耗时/机型），不上传图像

---

## 7. 数据结构与接口（建议）

### 7.1 游戏配置（可 JSON）

- `avatarAssets[]`：{ id, name, imgUrl, anchorPointsPreset? }
- 阈值配置：
    - `teethOpenThreshold`
    - `teethStableMs`
    - `fistThresholds`（按归一化距离）
    - `shakeSpeedThreshold`
    - `shakeWindowMs`
    - `shakePassRatio`

### 7.2 运行时事件（用于积分/埋点）

- `EVENT_FACE_DETECTED / LOST`
- `EVENT_TEETH_GATE_PASS`
- `EVENT_FIST_DETECTED`
- `EVENT_SHAKE_PASS`
- `EVENT_GAME_COMPLETE`

---

## 8. 测试方案（务实可落地）

### 8.1 功能测试用例

- 光照：强光/背光/昏暗
- 人脸：戴眼镜/口罩（应失败并提示）/不同角度
- 手势：空手晃动（应失败）/握拳慢摇（应失败）/握拳快摇（通过）
- 多人入镜：优先主脸（最大脸或最中心脸）

### 8.2 调试工具（强烈建议做）

- Debug Overlay：显示关键点、嘴部开合分数、握拳判定、速度曲线
- 阈值热更新：URL 参数或面板实时调参（极大加速你找阈值）

---

## 9. 迭代路线图

### V0（1~3 天能跑）

- FaceLandmarker 跟踪 + 头套 overlay
- TeethGate（张嘴到位）
- HandLandmarker + fist + shake
- 状态机 + 积分

### V1（更像 Pokemon Smile）

- 刷牙持续时间（例如 30s）+ 过程鼓励
- 更精细的“刷牙动作节奏”评分（频率、持续性）
- 音效、粒子动效、卡通角色反馈

### V2（“露牙”升级）

- mouth ROI + 轻量 TeethVisible 分类器（显著提升“露牙”语义准确度）
- 更严谨的防作弊策略

---

## 10. 跨平台迁移策略（iOS / Android / 桌面）

- MediaPipe Solutions/Tasks 本身强调多平台可用([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/guide?utm_source=chatgpt.com))
- 迁移重点不是模型，而是：
    1. 摄像头帧获取与渲染（各平台 API 不同）
    2. 同一套判定逻辑（建议抽成“platform-agnostic”模块：阈值、滑窗统计、状态机）
    3. 资源与权限（相机权限、前后台、性能档位）

---

# 你接下来可以直接开干的“最小任务清单”

1. 搭 Web 工程（Vite/React/TS）+ 摄像头预览
2. 接入 FaceLandmarker：拿到 landmarks + blendshapes([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
3. Canvas 叠加头套（2D 版本先跑通）
4. TeethGate：用 jawOpen/mouthOpen 分数 + 稳定时间门槛
5. 接入 HandLandmarker：拿手关键点与 handedness([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker?utm_source=chatgpt.com))
6. Fist + Shake：滑窗速度判定
7. 状态机串起来 + 积分展示 + debug 面板

---

## 2.X 工程框架与实现路线（新增）

### 2.X.1 Web MVP 推荐框架（结论）

**推荐：Vite + React + TypeScript + Canvas 2D（Overlay 渲染）**

选择理由：

- Vite 提供极快的本地启动与 HMR，适合你这种“摄像头实时效果 + 阈值调参”高频迭代开发。([vitejs](https://vite.dev/?utm_source=chatgpt.com))
- React + TS 便于组织“状态机（S0~S7）/调试面板/阈值配置/资源管理”，并能用 TS 类型约束让 AI 生成代码更稳定。
- MediaPipe Web JS 官方使用 `@mediapipe/tasks-vision` 路线（npm 安装或 CDN），与 Vite 工程化集成顺滑。([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))

> 备选：Vite + Svelte（更轻更快写 UI），但综合生态与长期扩展性，React 更稳。
> 

### 2.X.2 MediaPipe Web 接入方式（补充落地）

- 安装方式：优先使用 **NPM** 安装 `@mediapipe/tasks-vision`（锁版本、可控）([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
- 模型与 wasm：
    - Face Landmarker / Hand Landmarker 按官方示例加载 wasm root 与 modelAssetPath。([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
    - 模型文件建议放在项目静态资源目录（如 `public/models/...`），避免路径在构建后变化。

---

## 6.X 运行环境与“容易漏掉的坑”（新增检查清单）

### 6.X.1 摄像头权限与安全上下文（必须补齐）

- `getUserMedia()` **只能在安全上下文（HTTPS）**或本地 `localhost` 下稳定工作；非 HTTPS 环境会被浏览器限制/禁用。([MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia?utm_source=chatgpt.com))
    
    **结论**：你的 Web MVP 即使只是测试，也要用 `https://`（或 `localhost`）跑。
    

### 6.X.2 iOS / Safari 相关（强烈建议纳入）

- iPhone 上 `<video>` 建议加 `playsinline`，否则可能强制全屏播放，影响 Canvas 叠加与交互体验。([webkit.org](https://webkit.org/blog/6784/new-video-policies-for-ios/?utm_source=chatgpt.com))
- 自动播放/播放策略：在移动端（尤其 iOS）经常需要**用户手势**触发播放，建议 UI 流程把“开始游戏”按钮作为统一的用户手势入口。([MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Permissions-Policy/autoplay?utm_source=chatgpt.com))

### 6.X.3 主线程阻塞与 Web Worker（之前文档应补上）

- Face/Hand Landmarker 的 `detectForVideo()` 调用在 Web 上是**同步且会阻塞 UI 线程**；官方也明确建议用 **Web Worker** 把推理挪到子线程以减少卡顿。([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
    
    **建议（MVP 最小实现）**：
    
- 先主线程跑通闭环；一旦出现明显掉帧/卡 UI，再把推理循环移入 Worker（保持 API/数据结构不变，迁移成本最低）。

### 6.X.4 Cross-Origin Isolation（可选，但建议提前写进文档）

如果你后续想用 `SharedArrayBuffer`（例如更激进的多线程/性能方案），需要页面满足 **cross-origin isolated** 条件：

- `Cross-Origin-Opener-Policy: same-origin`
- `Cross-Origin-Embedder-Policy: require-corp` 或 `credentialless`
    
    并可用 `window.crossOriginIsolated` 检测。([MDN Web Docs](https://developer.mozilla.org/docs/Web/API/Window/crossOriginIsolated?utm_source=chatgpt.com))
    

> MVP 不强制开启，但建议在“部署/运维”章节标注：如果未来要做性能升级，这两组 Header 可能要上。
> 

### 6.X.5 WebAssembly SIMD 的现实约束（补充说明）

- MediaPipe Web 依赖 wasm 性能红利；多数现代浏览器支持 WebAssembly（含固定宽度 SIMD），但仍可能遇到“某些安全模式/锁定模式禁用 SIMD”等极端情况。([V8](https://v8.dev/features/simd?utm_source=chatgpt.com))
    
    **文档建议写法**：
    
- “性能与兼容性以最新 Chrome/Edge/Firefox/Safari 为基准；遇到极端模式降级到更低帧率或提示不支持。”

---

## 6.X.6 你开工前最后的“环境遗漏自检”清单（建议直接照此验收）

1. ✅ 本地开发是否在 `localhost` 或 HTTPS？（否则摄像头不稳）([MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia?utm_source=chatgpt.com))
2. ✅ `<video>` 是否设置 `playsinline`（移动端体验关键）([webkit.org](https://webkit.org/blog/6784/new-video-policies-for-ios/?utm_source=chatgpt.com))
3. ✅ “开始游戏”按钮是否作为用户手势入口（避免 autoplay 限制）([MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Permissions-Policy/autoplay?utm_source=chatgpt.com))
4. ✅ Face/Hand 推理是否按需降采样（每 2~3 帧跑一次）
5. ✅ 是否预留 Worker 化接口（detectForVideo 阻塞风险）([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
6. ✅ 模型与 wasm 路径在 build 后可用（静态资源目录/路径不变）([Google AI for Developers](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/web_js?utm_source=chatgpt.com))
7. ✅（可选）若未来要多线程/SharedArrayBuffer：是否具备 COOP/COEP 配置能力([MDN Web Docs](https://developer.mozilla.org/docs/Web/API/Window/crossOriginIsolated?utm_source=chatgpt.com))

---